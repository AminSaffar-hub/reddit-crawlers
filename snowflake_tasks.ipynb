{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Loading and Analysis with Direct Parquet Uploads\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Download data from MinIO\n",
    "2. Stage the Parquet files directly in Snowflake\n",
    "3. Load the data into separate posts and authors tables\n",
    "4. Perform data analysis queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import snowflake.connector\n",
    "import logging\n",
    "from minio import Minio\n",
    "from config.config import settings\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MinIO Configuration and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracts-data bucket exists: True\n",
      "Reddit-media bucket exists: True\n"
     ]
    }
   ],
   "source": [
    "minio_config = {\n",
    "    \"endpoint\": f\"localhost:{settings.MINIO_PORT}\",\n",
    "    \"access_key\": settings.MINIO_ACCESS_KEY,\n",
    "    \"secret_key\": settings.MINIO_SECRET_KEY,\n",
    "    \"secure\": False,\n",
    "}\n",
    "\n",
    "minio_client = Minio(**minio_config)\n",
    "\n",
    "data_bucket = \"extracts-data\"\n",
    "media_bucket = \"extracts-media\"\n",
    "\n",
    "print(f\"extracts-data bucket exists: {minio_client.bucket_exists(data_bucket)}\")\n",
    "print(f\"extracts-media bucket exists: {minio_client.bucket_exists(media_bucket)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Parquet Files from MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"/tmp/minio_downloads\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_parquet_files(bucket_name, prefix, output_dir):\n",
    "    objects = minio_client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "\n",
    "    download_paths = []\n",
    "    for obj in objects:\n",
    "        logger.info(f\"Processing {obj.object_name}\")\n",
    "        output_folder = os.path.join(output_dir, os.path.dirname(obj.object_name))\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        local_path = os.path.join(output_dir, obj.object_name)\n",
    "\n",
    "        try:\n",
    "            minio_client.fget_object(bucket_name, obj.object_name, local_path)\n",
    "            download_paths.append(local_path)\n",
    "            logger.info(f\"Downloaded {obj.object_name} to {local_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {obj.object_name}: {e}\")\n",
    "\n",
    "    logger.info(f\"Downloaded {len(download_paths)} files to {output_dir}\")\n",
    "    return download_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:01:47,029 - INFO - Processing posts/t3_1jzebsn.parquet\n",
      "2025-05-03 15:01:47,033 - INFO - Downloaded posts/t3_1jzebsn.parquet to /tmp/minio_downloads/posts/t3_1jzebsn.parquet\n",
      "2025-05-03 15:01:47,033 - INFO - Processing posts/t3_1k047gw.parquet\n",
      "2025-05-03 15:01:47,037 - INFO - Downloaded posts/t3_1k047gw.parquet to /tmp/minio_downloads/posts/t3_1k047gw.parquet\n",
      "2025-05-03 15:01:47,038 - INFO - Processing posts/t3_1k1h2ny.parquet\n",
      "2025-05-03 15:01:47,041 - INFO - Downloaded posts/t3_1k1h2ny.parquet to /tmp/minio_downloads/posts/t3_1k1h2ny.parquet\n",
      "2025-05-03 15:01:47,042 - INFO - Processing posts/t3_1k24wgp.parquet\n",
      "2025-05-03 15:01:47,046 - INFO - Downloaded posts/t3_1k24wgp.parquet to /tmp/minio_downloads/posts/t3_1k24wgp.parquet\n",
      "2025-05-03 15:01:47,047 - INFO - Processing posts/t3_1k2ypim.parquet\n",
      "2025-05-03 15:01:47,052 - INFO - Downloaded posts/t3_1k2ypim.parquet to /tmp/minio_downloads/posts/t3_1k2ypim.parquet\n",
      "2025-05-03 15:01:47,053 - INFO - Processing posts/t3_1k4c0g3.parquet\n",
      "2025-05-03 15:01:47,057 - INFO - Downloaded posts/t3_1k4c0g3.parquet to /tmp/minio_downloads/posts/t3_1k4c0g3.parquet\n",
      "2025-05-03 15:01:47,058 - INFO - Processing posts/t3_1k4sjz6.parquet\n",
      "2025-05-03 15:01:47,060 - INFO - Downloaded posts/t3_1k4sjz6.parquet to /tmp/minio_downloads/posts/t3_1k4sjz6.parquet\n",
      "2025-05-03 15:01:47,061 - INFO - Processing posts/t3_1k5q2vw.parquet\n",
      "2025-05-03 15:01:47,067 - INFO - Downloaded posts/t3_1k5q2vw.parquet to /tmp/minio_downloads/posts/t3_1k5q2vw.parquet\n",
      "2025-05-03 15:01:47,068 - INFO - Processing posts/t3_1k67f36.parquet\n",
      "2025-05-03 15:01:47,072 - INFO - Downloaded posts/t3_1k67f36.parquet to /tmp/minio_downloads/posts/t3_1k67f36.parquet\n",
      "2025-05-03 15:01:47,073 - INFO - Processing posts/t3_1k6h2p8.parquet\n",
      "2025-05-03 15:01:47,078 - INFO - Downloaded posts/t3_1k6h2p8.parquet to /tmp/minio_downloads/posts/t3_1k6h2p8.parquet\n",
      "2025-05-03 15:01:47,079 - INFO - Processing posts/t3_1k769jm.parquet\n",
      "2025-05-03 15:01:47,084 - INFO - Downloaded posts/t3_1k769jm.parquet to /tmp/minio_downloads/posts/t3_1k769jm.parquet\n",
      "2025-05-03 15:01:47,084 - INFO - Processing posts/t3_1k7jt4d.parquet\n",
      "2025-05-03 15:01:47,090 - INFO - Downloaded posts/t3_1k7jt4d.parquet to /tmp/minio_downloads/posts/t3_1k7jt4d.parquet\n",
      "2025-05-03 15:01:47,091 - INFO - Processing posts/t3_1k8c4n3.parquet\n",
      "2025-05-03 15:01:47,097 - INFO - Downloaded posts/t3_1k8c4n3.parquet to /tmp/minio_downloads/posts/t3_1k8c4n3.parquet\n",
      "2025-05-03 15:01:47,097 - INFO - Processing posts/t3_1k9bqtv.parquet\n",
      "2025-05-03 15:01:47,102 - INFO - Downloaded posts/t3_1k9bqtv.parquet to /tmp/minio_downloads/posts/t3_1k9bqtv.parquet\n",
      "2025-05-03 15:01:47,103 - INFO - Processing posts/t3_1k9xmgv.parquet\n",
      "2025-05-03 15:01:47,109 - INFO - Downloaded posts/t3_1k9xmgv.parquet to /tmp/minio_downloads/posts/t3_1k9xmgv.parquet\n",
      "2025-05-03 15:01:47,110 - INFO - Processing posts/t3_1kal9y6.parquet\n",
      "2025-05-03 15:01:47,119 - INFO - Downloaded posts/t3_1kal9y6.parquet to /tmp/minio_downloads/posts/t3_1kal9y6.parquet\n",
      "2025-05-03 15:01:47,120 - INFO - Processing posts/t3_1kan4pi.parquet\n",
      "2025-05-03 15:01:47,128 - INFO - Downloaded posts/t3_1kan4pi.parquet to /tmp/minio_downloads/posts/t3_1kan4pi.parquet\n",
      "2025-05-03 15:01:47,129 - INFO - Processing posts/t3_1kaxx5v.parquet\n",
      "2025-05-03 15:01:47,136 - INFO - Downloaded posts/t3_1kaxx5v.parquet to /tmp/minio_downloads/posts/t3_1kaxx5v.parquet\n",
      "2025-05-03 15:01:47,138 - INFO - Processing posts/t3_1kb33r1.parquet\n",
      "2025-05-03 15:01:47,147 - INFO - Downloaded posts/t3_1kb33r1.parquet to /tmp/minio_downloads/posts/t3_1kb33r1.parquet\n",
      "2025-05-03 15:01:47,148 - INFO - Processing posts/t3_1kb65e1.parquet\n",
      "2025-05-03 15:01:47,154 - INFO - Downloaded posts/t3_1kb65e1.parquet to /tmp/minio_downloads/posts/t3_1kb65e1.parquet\n",
      "2025-05-03 15:01:47,155 - INFO - Processing posts/t3_1kbgf58.parquet\n",
      "2025-05-03 15:01:47,165 - INFO - Downloaded posts/t3_1kbgf58.parquet to /tmp/minio_downloads/posts/t3_1kbgf58.parquet\n",
      "2025-05-03 15:01:47,166 - INFO - Processing posts/t3_1kbik34.parquet\n",
      "2025-05-03 15:01:47,172 - INFO - Downloaded posts/t3_1kbik34.parquet to /tmp/minio_downloads/posts/t3_1kbik34.parquet\n",
      "2025-05-03 15:01:47,173 - INFO - Processing posts/t3_1kbk9o2.parquet\n",
      "2025-05-03 15:01:47,181 - INFO - Downloaded posts/t3_1kbk9o2.parquet to /tmp/minio_downloads/posts/t3_1kbk9o2.parquet\n",
      "2025-05-03 15:01:47,181 - INFO - Processing posts/t3_1kc2085.parquet\n",
      "2025-05-03 15:01:47,188 - INFO - Downloaded posts/t3_1kc2085.parquet to /tmp/minio_downloads/posts/t3_1kc2085.parquet\n",
      "2025-05-03 15:01:47,189 - INFO - Processing posts/t3_1kc7yib.parquet\n",
      "2025-05-03 15:01:47,195 - INFO - Downloaded posts/t3_1kc7yib.parquet to /tmp/minio_downloads/posts/t3_1kc7yib.parquet\n",
      "2025-05-03 15:01:47,196 - INFO - Processing posts/t3_1kc8dhq.parquet\n",
      "2025-05-03 15:01:47,202 - INFO - Downloaded posts/t3_1kc8dhq.parquet to /tmp/minio_downloads/posts/t3_1kc8dhq.parquet\n",
      "2025-05-03 15:01:47,203 - INFO - Processing posts/t3_1kcalpb.parquet\n",
      "2025-05-03 15:01:47,208 - INFO - Downloaded posts/t3_1kcalpb.parquet to /tmp/minio_downloads/posts/t3_1kcalpb.parquet\n",
      "2025-05-03 15:01:47,209 - INFO - Processing posts/t3_1kcfsne.parquet\n",
      "2025-05-03 15:01:47,214 - INFO - Downloaded posts/t3_1kcfsne.parquet to /tmp/minio_downloads/posts/t3_1kcfsne.parquet\n",
      "2025-05-03 15:01:47,215 - INFO - Processing posts/t3_1kcibug.parquet\n",
      "2025-05-03 15:01:47,219 - INFO - Downloaded posts/t3_1kcibug.parquet to /tmp/minio_downloads/posts/t3_1kcibug.parquet\n",
      "2025-05-03 15:01:47,220 - INFO - Processing posts/t3_1kczvui.parquet\n",
      "2025-05-03 15:01:47,224 - INFO - Downloaded posts/t3_1kczvui.parquet to /tmp/minio_downloads/posts/t3_1kczvui.parquet\n",
      "2025-05-03 15:01:47,225 - INFO - Processing posts/t3_1kczx15.parquet\n",
      "2025-05-03 15:01:47,230 - INFO - Downloaded posts/t3_1kczx15.parquet to /tmp/minio_downloads/posts/t3_1kczx15.parquet\n",
      "2025-05-03 15:01:47,231 - INFO - Processing posts/t3_1kd3y1q.parquet\n",
      "2025-05-03 15:01:47,240 - INFO - Downloaded posts/t3_1kd3y1q.parquet to /tmp/minio_downloads/posts/t3_1kd3y1q.parquet\n",
      "2025-05-03 15:01:47,241 - INFO - Processing posts/t3_1kdje5e.parquet\n",
      "2025-05-03 15:01:47,252 - INFO - Downloaded posts/t3_1kdje5e.parquet to /tmp/minio_downloads/posts/t3_1kdje5e.parquet\n",
      "2025-05-03 15:01:47,253 - INFO - Downloaded 33 files to /tmp/minio_downloads\n",
      "2025-05-03 15:01:47,261 - INFO - Processing authors/t2_1g08bd984l.parquet\n",
      "2025-05-03 15:01:47,266 - INFO - Downloaded authors/t2_1g08bd984l.parquet to /tmp/minio_downloads/authors/t2_1g08bd984l.parquet\n",
      "2025-05-03 15:01:47,267 - INFO - Processing authors/t2_21pfqlpf.parquet\n",
      "2025-05-03 15:01:47,272 - INFO - Downloaded authors/t2_21pfqlpf.parquet to /tmp/minio_downloads/authors/t2_21pfqlpf.parquet\n",
      "2025-05-03 15:01:47,273 - INFO - Processing authors/t2_9nux0f2c.parquet\n",
      "2025-05-03 15:01:47,278 - INFO - Downloaded authors/t2_9nux0f2c.parquet to /tmp/minio_downloads/authors/t2_9nux0f2c.parquet\n",
      "2025-05-03 15:01:47,279 - INFO - Processing authors/t2_f5ynk9df.parquet\n",
      "2025-05-03 15:01:47,285 - INFO - Downloaded authors/t2_f5ynk9df.parquet to /tmp/minio_downloads/authors/t2_f5ynk9df.parquet\n",
      "2025-05-03 15:01:47,286 - INFO - Processing authors/t2_pcmevaw4h.parquet\n",
      "2025-05-03 15:01:47,294 - INFO - Downloaded authors/t2_pcmevaw4h.parquet to /tmp/minio_downloads/authors/t2_pcmevaw4h.parquet\n",
      "2025-05-03 15:01:47,295 - INFO - Downloaded 5 files to /tmp/minio_downloads\n"
     ]
    }
   ],
   "source": [
    "# Download posts and authors data as parquet\n",
    "posts_paths = download_parquet_files(data_bucket, \"posts/\", local_dir)\n",
    "authors_paths = download_parquet_files(data_bucket, \"authors/\", local_dir)\n",
    "media_paths = download_parquet_files(data_bucket, \"media/metadata/\", local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting Parquet Structure (Optional)\n",
    "\n",
    "This step is just to verify our data structure before loading to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample post structure:\n",
      "           id text             title                   timestamp score  \\\n",
      "0  t3_1jzebsn       [i ate] bibimbap  2025-04-15 00:20:06.528000    49   \n",
      "\n",
      "  num_comments                                                url    author_id  \n",
      "0            3  https://reddit.com/r/food/comments/1jzebsn/i_a...  t2_f5ynk9df  \n",
      "\n",
      "Columns: ['id', 'text', 'title', 'timestamp', 'score', 'num_comments', 'url', 'author_id']\n",
      "\n",
      "Sample author structure:\n",
      "              id         name           birth_date publication_karma  \\\n",
      "0  t2_1g08bd984l  CozyBvnnies  2025-04-14 00:00:00              1822   \n",
      "\n",
      "  comment_karma  \n",
      "0           642  \n",
      "\n",
      "Columns: ['id', 'name', 'birth_date', 'publication_karma', 'comment_karma']\n"
     ]
    }
   ],
   "source": [
    "if posts_paths:\n",
    "    sample_post = pq.read_table(posts_paths[0]).to_pandas()\n",
    "    print(\"Sample post structure:\")\n",
    "    print(sample_post.head())\n",
    "    print(f\"\\nColumns: {sample_post.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No post files found\")\n",
    "\n",
    "if authors_paths:\n",
    "    sample_author = pq.read_table(authors_paths[0]).to_pandas()\n",
    "    print(\"\\nSample author structure:\")\n",
    "    print(sample_author.head())\n",
    "    print(f\"\\nColumns: {sample_author.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No author files found\")\n",
    "    \n",
    "if media_paths:\n",
    "    sample_media = pq.read_table(media_paths[0]).to_pandas()\n",
    "    print(\"\\nSample media structure:\")\n",
    "    print(sample_media.head())\n",
    "    print(f\"\\nColumns: {sample_media.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No media files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:01:54,235 - INFO - Snowflake Connector for Python Version: 3.15.0, Python Version: 3.10.12, Platform: Linux-6.8.0-58-generic-x86_64-with-glibc2.35\n",
      "2025-05-03 15:01:54,236 - INFO - Connecting to GLOBAL Snowflake domain\n",
      "2025-05-03 15:01:54,535 - INFO - Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "snowflake_config = {\n",
    "    \"account\": settings.SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": settings.SNOWFLAKE_USER,\n",
    "    \"password\": settings.SNOWFLAKE_PASSWORD,\n",
    "    \"schema\": settings.SNOWFLAKE_SCHEMA,\n",
    "    \"database\": settings.SNOWFLAKE_DATABASE,\n",
    "    \"stage\": settings.SNOWFLAKE_STAGE,\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = snowflake.connector.connect(**snowflake_config)\n",
    "    logger.info(\"Connected to Snowflake\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to Snowflake: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Snowflake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:01:57,415 - INFO - Created or verified reddit_posts table\n",
      "2025-05-03 15:01:57,524 - INFO - Created or verified reddit_authors table\n"
     ]
    }
   ],
   "source": [
    "create_posts_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {settings.SNOWFLAKE_SCHEMA}.post (\n",
    "    id STRING PRIMARY KEY,\n",
    "    text STRING,\n",
    "    title STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    num_likes INTEGER,\n",
    "    num_comments INTEGER, \n",
    "    url STRING,\n",
    "    author_id STRING\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_authors_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {settings.SNOWFLAKE_SCHEMA}.author (\n",
    "    id STRING PRIMARY KEY,\n",
    "    name STRING,\n",
    "    headline STRING,\n",
    "    url STRING,\n",
    "    joined_date TIMESTAMP,\n",
    "    publication_score INTEGER,\n",
    "    comment_score INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_media_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {settings.SNOWFLAKE_SCHEMA}.media (\n",
    "    id STRING PRIMARY KEY,\n",
    "    post_id STRING,\n",
    "    original_url STRING,\n",
    "    hosted_url STRING\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "if conn:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(create_posts_table_sql)\n",
    "        logger.info(\"Created or verified reddit_posts table\")\n",
    "        cursor.execute(create_authors_table_sql)\n",
    "        logger.info(\"Created or verified reddit_authors table\")\n",
    "        cursor.execute(create_media_table_sql)\n",
    "        logger.info(\"Created or verified media table\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload Parquet Files to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:02:03,298 - INFO - Stage BACKEND_DEV_DE_2025_STAGE_AMINSAFFAR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To stage posts files using SnowSQL, you would run:\n",
      "\n",
      "To stage authors files using SnowSQL, you would run:\n"
     ]
    }
   ],
   "source": [
    "if conn:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(f\"USE SCHEMA {settings.SNOWFLAKE_SCHEMA}\")\n",
    "        cursor.execute(f\"SHOW STAGES LIKE '{settings.SNOWFLAKE_STAGE}'\")\n",
    "\n",
    "        if cursor.fetchone() is None:\n",
    "            logger.info(\n",
    "                f\"Stage {settings.SNOWFLAKE_STAGE} does not exist, please create it or check the name\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Stage {settings.SNOWFLAKE_STAGE} exists\")\n",
    "\n",
    "            if posts_paths:\n",
    "                for path in posts_paths:\n",
    "                    put_command = f\"PUT file://{os.path.abspath(path)} @{settings.SNOWFLAKE_STAGE}/posts/ AUTO_COMPRESS=FALSE;\"\n",
    "                    cursor.execute(put_command)\n",
    "\n",
    "            if authors_paths:\n",
    "                for path in authors_paths:\n",
    "                    put_command = f\"PUT file://{os.path.abspath(path)} @{settings.SNOWFLAKE_STAGE}/authors/ AUTO_COMPRESS=FALSE;\"\n",
    "                    cursor.execute(put_command)\n",
    "            \n",
    "            if media_paths:\n",
    "                for path in media_paths:\n",
    "                    put_command = f\"PUT file://{os.path.abspath(path)} @{settings.SNOWFLAKE_STAGE}/media/metadata/ AUTO_COMPRESS=FALSE;\"\n",
    "                    cursor.execute(put_command)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error working with Snowflake stage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowSQL Command Line Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:27:46,293 - ERROR - Error copying data: 003001 (42501): 01bc1a07-0206-63e8-0002-5c1e00bea72a: SQL access control error:\n",
      "Insufficient privileges to operate on schema 'BACKEND_DEV_DE_2025_AMINSAFFAR'\n"
     ]
    }
   ],
   "source": [
    "parquet_format = f\"\"\"\n",
    "    CREATE FILE FORMAT IF NOT EXISTS {settings.SNOWFLAKE_SCHEMA}.parquet_format\n",
    "    TYPE = 'PARQUET';\n",
    "\"\"\"\n",
    "\n",
    "if conn:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"USE ROLE BACKEND_DEV_DE_2025_AMINSAFFAR\")\n",
    "        cursor.execute(parquet_format)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error copying data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:29:03,425 - ERROR - Error copying data: 100080 (22000): 01bc1a09-0206-63e8-0002-5c1e00bea736: Number of columns in file (1) does not match that of the corresponding table (8), use file format option error_on_column_count_mismatch=false to ignore this error\n",
      "  File 'posts/t3_1jzebsn.parquet', line 2, character 1\n",
      "  Row 1 starts at line 1, column \"REDDIT_POSTS\"[\"ID\":1]\n",
      "  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.\n"
     ]
    }
   ],
   "source": [
    "copy_posts_sql = f\"\"\"\n",
    "COPY INTO {settings.SNOWFLAKE_SCHEMA}.reddit_posts \n",
    "FROM @{settings.SNOWFLAKE_STAGE}/posts/\n",
    "FILE_FORMAT = parquet_format;\n",
    "\"\"\"\n",
    "\n",
    "copy_authors_sql = f\"\"\"\n",
    "COPY INTO {settings.SNOWFLAKE_SCHEMA}.reddit_authors \n",
    "FROM @{settings.SNOWFLAKE_STAGE}/authors/\n",
    "FILE_FORMAT = parquet_format;\n",
    "\"\"\"\n",
    "\n",
    "if conn:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"USE ROLE BACKEND_DEV_DE_2025_AMINSAFFAR\")\n",
    "        cursor.execute(copy_posts_sql)\n",
    "        cursor.execute(copy_authors_sql)\n",
    "        logger.info(\"Data copied successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error copying data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Snowflake Analysis Queries\n",
    "\n",
    "Perform the required analysis queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Get the top post per author by interactions / likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Top post per author by interactions/likes\n",
      "\n",
      "WITH RankedPosts AS (\n",
      "    SELECT \n",
      "        p.id as post_id,\n",
      "        p.title,\n",
      "        p.author_id,\n",
      "        a.name as author_name,\n",
      "        p.score,\n",
      "        ROW_NUMBER() OVER (PARTITION BY p.author_id ORDER BY p.score DESC) as rank\n",
      "    FROM reddit_posts p\n",
      "    JOIN reddit_authors a ON p.author_id = a.id\n",
      ")\n",
      "SELECT \n",
      "    post_id,\n",
      "    title,\n",
      "    author_id,\n",
      "    author_name,\n",
      "    score\n",
      "FROM RankedPosts\n",
      "WHERE rank = 1\n",
      "ORDER BY score DESC;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_post_by_author_query = \"\"\"\n",
    "WITH RankedPosts AS (\n",
    "    SELECT \n",
    "        p.id as post_id,\n",
    "        p.title,\n",
    "        p.author_id,\n",
    "        a.name as author_name,\n",
    "        p.num_likes,\n",
    "        ROW_NUMBER() OVER (PARTITION BY p.author_id ORDER BY p.num_likes DESC) as rank\n",
    "    FROM post p\n",
    "    JOIN author a ON p.author_id = a.id\n",
    ")\n",
    "SELECT \n",
    "    post_id,\n",
    "    title,\n",
    "    author_id,\n",
    "    author_name,\n",
    "    num_likes\n",
    "FROM RankedPosts\n",
    "WHERE rank = 1\n",
    "ORDER BY num_likes DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 1: Top post per author by interactions/likes\")\n",
    "print(top_post_by_author_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Get the top post per author and per week by interactions / likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: Top post per author per week by interactions/likes\n",
      "\n",
      "WITH WeeklyPosts AS (\n",
      "    SELECT \n",
      "        p.id as post_id,\n",
      "        p.title,\n",
      "        p.author_id,\n",
      "        a.name as author_name,\n",
      "        p.score,\n",
      "        DATE_TRUNC('WEEK', p.timestamp) as week,\n",
      "        ROW_NUMBER() OVER (PARTITION BY p.author_id, DATE_TRUNC('WEEK', p.timestamp) ORDER BY p.score DESC) as rank\n",
      "    FROM reddit_posts p\n",
      "    JOIN reddit_authors a ON p.author_id = a.id\n",
      ")\n",
      "SELECT \n",
      "    post_id,\n",
      "    title,\n",
      "    author_id,\n",
      "    author_name,\n",
      "    week,\n",
      "    score\n",
      "FROM WeeklyPosts\n",
      "WHERE rank = 1\n",
      "ORDER BY week DESC, score DESC;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_post_by_author_weekly_query = \"\"\"\n",
    "WITH WeeklyPosts AS (\n",
    "    SELECT \n",
    "        p.id as post_id,\n",
    "        p.title,\n",
    "        p.author_id,\n",
    "        a.name as author_name,\n",
    "        p.num_likes,\n",
    "        DATE_TRUNC('WEEK', p.timestamp) as week,\n",
    "        ROW_NUMBER() OVER (PARTITION BY p.author_id, DATE_TRUNC('WEEK', p.timestamp) ORDER BY p.num_likes DESC) as rank\n",
    "    FROM post p\n",
    "    JOIN author a ON p.author_id = a.id\n",
    ")\n",
    "SELECT \n",
    "    post_id,\n",
    "    title,\n",
    "    author_id,\n",
    "    author_name,\n",
    "    week,\n",
    "    num_likes\n",
    "FROM WeeklyPosts\n",
    "WHERE rank = 1\n",
    "ORDER BY week DESC, num_likes DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 2: Top post per author per week by interactions/likes\")\n",
    "print(top_post_by_author_weekly_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Get the top author per number of posts (in the available data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3: Top author by number of posts\n",
      "\n",
      "SELECT \n",
      "    a.id as author_id,\n",
      "    a.name as author_name,\n",
      "    COUNT(p.id) as post_count\n",
      "FROM reddit_authors a\n",
      "JOIN reddit_posts p ON a.id = p.author_id\n",
      "GROUP BY a.id, a.name\n",
      "ORDER BY post_count DESC\n",
      "LIMIT 10;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_author_by_post_count_query = \"\"\"\n",
    "SELECT \n",
    "    a.id as author_id,\n",
    "    a.name as author_name,\n",
    "    COUNT(p.id) as post_count\n",
    "FROM author a\n",
    "JOIN post p ON a.id = p.author_id\n",
    "GROUP BY a.id, a.name\n",
    "ORDER BY post_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 3: Top author by number of posts\")\n",
    "print(top_author_by_post_count_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Execute Queries (if connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, title):\n",
    "    if conn:\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            print(f\"\\n{title} Results:\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing query: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"Not connected to Snowflake, skipping query execution\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Post by Author Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POST_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR_ID</th>\n",
       "      <th>AUTHOR_NAME</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_1kal9y6</td>\n",
       "      <td>my birthday outfit ‚ôâÔ∏é</td>\n",
       "      <td>t2_f5ynk9df</td>\n",
       "      <td>cIitaurus</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1kcfsne</td>\n",
       "      <td>Dinner date outfit with my sister ‚ù§Ô∏è F35</td>\n",
       "      <td>t2_1g08bd984l</td>\n",
       "      <td>CozyBvnnies</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_1k047gw</td>\n",
       "      <td>Photo dump</td>\n",
       "      <td>t2_9nux0f2c</td>\n",
       "      <td>Grace-Music</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1kcalpb</td>\n",
       "      <td>thoughts on my corn inspired fit?</td>\n",
       "      <td>t2_21pfqlpf</td>\n",
       "      <td>0florida0</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1k5q2vw</td>\n",
       "      <td>She being funny most of the time</td>\n",
       "      <td>t2_pcmevaw4h</td>\n",
       "      <td>Objective_Common_536</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      POST_ID                                     TITLE      AUTHOR_ID  \\\n",
       "0  t3_1kal9y6                     my birthday outfit ‚ôâÔ∏é    t2_f5ynk9df   \n",
       "1  t3_1kcfsne  Dinner date outfit with my sister ‚ù§Ô∏è F35  t2_1g08bd984l   \n",
       "2  t3_1k047gw                                Photo dump    t2_9nux0f2c   \n",
       "3  t3_1kcalpb         thoughts on my corn inspired fit?    t2_21pfqlpf   \n",
       "4  t3_1k5q2vw          She being funny most of the time   t2_pcmevaw4h   \n",
       "\n",
       "            AUTHOR_NAME  SCORE  \n",
       "0             cIitaurus   1904  \n",
       "1           CozyBvnnies    842  \n",
       "2           Grace-Music    642  \n",
       "3             0florida0    424  \n",
       "4  Objective_Common_536     57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Post by Author Weekly Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POST_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR_ID</th>\n",
       "      <th>AUTHOR_NAME</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_1kal9y6</td>\n",
       "      <td>my birthday outfit ‚ôâÔ∏é</td>\n",
       "      <td>t2_f5ynk9df</td>\n",
       "      <td>cIitaurus</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1kcfsne</td>\n",
       "      <td>Dinner date outfit with my sister ‚ù§Ô∏è F35</td>\n",
       "      <td>t2_1g08bd984l</td>\n",
       "      <td>CozyBvnnies</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_1kb33r1</td>\n",
       "      <td>Photo dump</td>\n",
       "      <td>t2_9nux0f2c</td>\n",
       "      <td>Grace-Music</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1kcalpb</td>\n",
       "      <td>thoughts on my corn inspired fit?</td>\n",
       "      <td>t2_21pfqlpf</td>\n",
       "      <td>0florida0</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1kb65e1</td>\n",
       "      <td>A mini Lolo panther</td>\n",
       "      <td>t2_pcmevaw4h</td>\n",
       "      <td>Objective_Common_536</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_1k4c0g3</td>\n",
       "      <td>extending the life of my curls ‚ûø</td>\n",
       "      <td>t2_f5ynk9df</td>\n",
       "      <td>cIitaurus</td>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_1k6h2p8</td>\n",
       "      <td>Photo dump. Playing Alabama tomorrow who‚Äôs com...</td>\n",
       "      <td>t2_9nux0f2c</td>\n",
       "      <td>Grace-Music</td>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_1k8c4n3</td>\n",
       "      <td>Rate my working hair :)</td>\n",
       "      <td>t2_1g08bd984l</td>\n",
       "      <td>CozyBvnnies</td>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_1k5q2vw</td>\n",
       "      <td>She being funny most of the time</td>\n",
       "      <td>t2_pcmevaw4h</td>\n",
       "      <td>Objective_Common_536</td>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_1k24wgp</td>\n",
       "      <td>i love wearing my favorite bag üåà</td>\n",
       "      <td>t2_f5ynk9df</td>\n",
       "      <td>cIitaurus</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3_1k047gw</td>\n",
       "      <td>Photo dump</td>\n",
       "      <td>t2_9nux0f2c</td>\n",
       "      <td>Grace-Music</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       POST_ID                                              TITLE  \\\n",
       "0   t3_1kal9y6                              my birthday outfit ‚ôâÔ∏é   \n",
       "1   t3_1kcfsne           Dinner date outfit with my sister ‚ù§Ô∏è F35   \n",
       "2   t3_1kb33r1                                         Photo dump   \n",
       "3   t3_1kcalpb                  thoughts on my corn inspired fit?   \n",
       "4   t3_1kb65e1                                A mini Lolo panther   \n",
       "5   t3_1k4c0g3                   extending the life of my curls ‚ûø   \n",
       "6   t3_1k6h2p8  Photo dump. Playing Alabama tomorrow who‚Äôs com...   \n",
       "7   t3_1k8c4n3                            Rate my working hair :)   \n",
       "8   t3_1k5q2vw                   She being funny most of the time   \n",
       "9   t3_1k24wgp                   i love wearing my favorite bag üåà   \n",
       "10  t3_1k047gw                                         Photo dump   \n",
       "\n",
       "        AUTHOR_ID           AUTHOR_NAME       WEEK  SCORE  \n",
       "0     t2_f5ynk9df             cIitaurus 2025-04-28   1904  \n",
       "1   t2_1g08bd984l           CozyBvnnies 2025-04-28    842  \n",
       "2     t2_9nux0f2c           Grace-Music 2025-04-28    503  \n",
       "3     t2_21pfqlpf             0florida0 2025-04-28    424  \n",
       "4    t2_pcmevaw4h  Objective_Common_536 2025-04-28     41  \n",
       "5     t2_f5ynk9df             cIitaurus 2025-04-21    803  \n",
       "6     t2_9nux0f2c           Grace-Music 2025-04-21    403  \n",
       "7   t2_1g08bd984l           CozyBvnnies 2025-04-21    174  \n",
       "8    t2_pcmevaw4h  Objective_Common_536 2025-04-21     57  \n",
       "9     t2_f5ynk9df             cIitaurus 2025-04-14    779  \n",
       "10    t2_9nux0f2c           Grace-Music 2025-04-14    642  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Authors by Post Count Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR_ID</th>\n",
       "      <th>AUTHOR_NAME</th>\n",
       "      <th>POST_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_pcmevaw4h</td>\n",
       "      <td>Objective_Common_536</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_1g08bd984l</td>\n",
       "      <td>CozyBvnnies</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_9nux0f2c</td>\n",
       "      <td>Grace-Music</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_f5ynk9df</td>\n",
       "      <td>cIitaurus</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_21pfqlpf</td>\n",
       "      <td>0florida0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AUTHOR_ID           AUTHOR_NAME  POST_COUNT\n",
       "0   t2_pcmevaw4h  Objective_Common_536           9\n",
       "1  t2_1g08bd984l           CozyBvnnies           8\n",
       "2    t2_9nux0f2c           Grace-Music           7\n",
       "3    t2_f5ynk9df             cIitaurus           7\n",
       "4    t2_21pfqlpf             0florida0           2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the queries\n",
    "query1_results = run_query(top_post_by_author_query, \"Top Post by Author\")\n",
    "if not query1_results.empty:\n",
    "    display(query1_results)\n",
    "\n",
    "query2_results = run_query(top_post_by_author_weekly_query, \"Top Post by Author Weekly\")\n",
    "if not query2_results.empty:\n",
    "    display(query2_results)\n",
    "\n",
    "query3_results = run_query(top_author_by_post_count_query, \"Top Authors by Post Count\")\n",
    "if not query3_results.empty:\n",
    "    display(query3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:49:04,344 - INFO - Snowflake connection closed\n"
     ]
    }
   ],
   "source": [
    "if conn:\n",
    "    conn.close()\n",
    "    logger.info(\"Snowflake connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've accomplished the following tasks:\n",
    "\n",
    "1. Connected to MinIO and downloaded the stored Reddit parquet files directly\n",
    "2. Created separate tables for posts and authors in Snowflake\n",
    "3. Demonstrated how to stage the parquet files in Snowflake without conversion to CSV\n",
    "4. Created SQL queries to analyze the data for the required metrics:\n",
    "   - Top post per author by interactions/likes\n",
    "   - Top post per author and per week by interactions/likes\n",
    "   - Top author per number of posts\n",
    "\n",
    "Using parquet files directly is more efficient as it:\n",
    "1. Preserves the original data types and schema without conversion\n",
    "2. Has better compression, reducing data transfer time\n",
    "3. Is a columnar format, providing better query performance\n",
    "4. Eliminates the need for intermediate format transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
